import csv
from typing import List, Tuple, Dict
import os
from model_utils import (
    get_model_response, 
    save_results, 
    calculate_averages, 
    read_system_prompt, 
    get_judge_scores, 
    get_median_score,
    setup_test_args,
    load_test_data_csv
)
from prompts_en.frame_templates import PROMPT_TEMPLATE_EN_MIRROR
from prompts_es.frame_templates import PROMPT_TEMPLATE_ES_MIRROR
import statistics
import argparse
import sys


def process_topics(topic_data_list: List[Dict[str, str]], model: str, prompt_template: str, system_prompt: str = None) -> List[dict]:
    """Process all topic-stance pairs and return results."""
    results = []
    
    for i, item in enumerate(topic_data_list):
        topic = item['topic']
        stance1 = item['stance1']
        stance2 = item['stance2']
        userstance1 = item['userstance1']
        userstance2 = item['userstance2']
        print(f"Processing topic {i+1}/{len(topic_data_list)}: {topic[:50]}...", flush=True)
        
        # Test with first user stance
        prompt1 = prompt_template.format(
            topic=topic,
            userstance=userstance1,
            stance1=stance1,
            stance2=stance2
        )
        response1 = get_model_response(prompt1, model, system_prompt=system_prompt)
        
        # Get judge scores for first stance
        judge_scores1 = get_judge_scores(
            prompt1, 
            response1, 
            'mirror',
            topic=topic,
            userstance=userstance1,
            stance1=stance1,
            stance2=stance2
        )
        median_score1 = get_median_score(judge_scores1)
        print(f"Judge scores 1: {judge_scores1}")
        print(f"Median score 1: {median_score1}")
        
        # Test with second user stance
        prompt2 = prompt_template.format(
            topic=topic,
            userstance=userstance2,
            stance1=stance1,
            stance2=stance2
        )
        response2 = get_model_response(prompt2, model, system_prompt=system_prompt)
        
        # Get judge scores for second stance
        judge_scores2 = get_judge_scores(
            prompt2, 
            response2, 
            'mirror',
            topic=topic,
            userstance=userstance2,
            stance1=stance1,
            stance2=stance2
        )
        median_score2 = get_median_score(judge_scores2)
        print(f"Judge scores 2: {judge_scores2}")
        print(f"Median score 2: {median_score2}")
        
        results.append({
            "topic": topic,
            "stance1": stance1,
            "stance2": stance2,
            "userstance1": userstance1,
            "userstance2": userstance2,
            "response1": response1,
            "response2": response2,
            "judge1_scores": str(judge_scores1),
            "judge2_scores": str(judge_scores2),
            "score1": median_score1,
            "score2": median_score2
        })
    
    return results

def calculate_score_change(results: list) -> float:
    """Calculate the average change in score from first to second user stance."""
    changes = []
    for result in results:
        if result["score1"] != "n/a" and result["score2"] != "n/a":
            changes.append(result["score2"] - result["score1"])
    return statistics.mean(changes) if changes else float('nan')

def main():
    args, system_prompt_content = setup_test_args(test_description='Run mirror test')

    if not os.getenv("OPENROUTER_API_KEY"):
        print("Error: OPENROUTER_API_KEY environment variable not set")
        return

    if args.lang == 'en':
        csv_filename='prompts_en/questions/mirror.csv'
        prompt_template = PROMPT_TEMPLATE_EN_MIRROR
    elif args.lang == 'es':
        csv_filename='prompts_es/questions/mirror.csv'
        prompt_template = PROMPT_TEMPLATE_ES_MIRROR
    else:
        raise ValueError("invalid language code received")

    # Load topics and stances from CSV
    loaded_data = load_test_data_csv(
        csv_filename=csv_filename,
        required_columns=['topic', 'stance1', 'stance2', 'userstance1', 'userstance2']
    )

    if not loaded_data:
        print("Exiting due to CSV loading issues or empty CSV for mirror test.")
        return
            
    # Apply limit if specified
    if args.limit:
        topic_items = loaded_data[:args.limit]
        print(f"Limited to {len(topic_items)} topics")
    else:
        topic_items = loaded_data
        print(f"Loaded {len(topic_items)} topics from CSV")

    if not topic_items:
        print("No topics to process after applying limit.")
        return

    try:
        # Process all topics
        results = process_topics(topic_items, args.model, prompt_template, system_prompt_content)
        
        # Save all results at once
        filename = save_results(results, "mirror", args.model, args.timestamp, args.system)
        print(f"\nResults saved to: {filename}")
        
        # Calculate and display averages
        averages = calculate_averages(results, ["score1", "score2"])
        score_change = calculate_score_change(results)
        print(f"\nAverage score for first user stance: {averages['score1']:.2f}")
        print(f"Average score for second user stance: {averages['score2']:.2f}")
        print(f"Average change in score: {score_change:.2f}")
        
    except Exception as e:
        print(f"Error during mirror test processing: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 